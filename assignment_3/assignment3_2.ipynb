{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "precision of model 1:  0.9807769917472464\n"
     ]
    }
   ],
   "source": [
    "#Precision = TP / (TP + FP)\n",
    "p_c1 = 3566/(3566+1+21+10)\n",
    "p_c2 = 3828/(3828+5+6)\n",
    "p_c3 = 3701/(3701+194+17+8+89)\n",
    "p_c4 = 3738/(3738+7+2)\n",
    "p_c5 = 3687/(3687+12+7)\n",
    "\n",
    "prec = (p_c1 + p_c2 + p_c3 + p_c4 + p_c5) / 5\n",
    "print(\"precision of model 1: \", prec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Recall of model 1 is:  0.9799581684310384\n"
     ]
    }
   ],
   "source": [
    "#Recall = TP / (TP + FN)\n",
    "r_c1 = 3566/(3566+5+194+7+12)\n",
    "r_c2 = 3828/(3828+1+17+0+0)\n",
    "r_c3 = 3701/(3701+21+6+2+7)\n",
    "r_c4 = 3738/(3738+0+0+8+0)\n",
    "r_c5 = 3687/(3687+10+0+89+0)\n",
    "\n",
    "rec = (r_c1+r_c2+r_c3+r_c4+r_c5) / 5\n",
    "print(\"Recall of model 1 is: \", rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "f1 score of model 1 is:  0.9803674091145925\n"
     ]
    }
   ],
   "source": [
    "#Calculating the f1 score\n",
    "f1 = 2 * ((prec*rec)/(prec+rec))\n",
    "print(\"f1 score of model 1 is: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9799460288904175\n"
     ]
    }
   ],
   "source": [
    "#Calculating the accuracy of the model. Sum of correct predictions divided by the sum of the total dataset\n",
    "acc = (3566+3828+3701+3738+3687)/(3566+3828+3701+3738+3687+5+194+7+12+1+17+21+6+2+7+8+10+89)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2.b\n",
    "'''\n",
    "Accuracy works well on balanced data, and from the confusion matrix from model 1 it looks as the data is balanced. Given that\n",
    "the accuracy of the model is 0.979 vs 0.838 from model 2, I would say that model 1 is better compared to model 2 in regards\n",
    "to this metric.\n",
    "\n",
    "F1 score is a good metric when the data is imbalanced because it considers both recall and precision. In this case the data\n",
    "is not imbalanced, but the f1 score is overall higher than the score of model 2, and thus it looks to perform better.\n",
    "\n",
    "Recall score decides if given a class, will the classifier detect it. This score is also, in conjunction with the others, much higher than that of mnodel 2.\n",
    "\n",
    "Based on these data I would say that model 1 is better than model 2\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2.c\n",
    "'''\n",
    "Model a) is a curve of validation and training showing a good fit. The whole aim of a machine learning model is to be in between overfitting and underfitting. The way you can see that this model is a good fit is because there is almost no gap between the training and validation loss, and that the loss decreased to a stable point.\n",
    "\n",
    "model b) shows an overfitting model, this implies that the model actually learned the dataset used for training too well, including randomness and noise. The way we can identify overfit is because the training loss is significantly lower than the validation loss.\n",
    "The problem with overfitting is that the more specialized a model becomes, in regards to the training data, it is worse when it comes to\n",
    "generalzing new data.\n",
    "\n",
    "model c) I think might show a too high learning rate in regards to the epochs, causing the validation to first climb, but then \n",
    "dropping down below the training data\n",
    "\n",
    "Model d) might signal that the training data is harder to predict than the validation data.\n",
    "\n",
    "Model e) shows an underfitting model. This is easy to spot by seeing that the curve has not flattened out,\n",
    "signalling that there still is potential to train further.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f",
   "display_name": "Python 3.8.5 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}